{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "widespread-genetics",
   "metadata": {},
   "source": [
    "### Instalação do XGBoost\n",
    "\n",
    "1. Vá em File -> New -> Terminal e digite o seguinte comando:\n",
    "\n",
    "> conda install -c conda-forge xgboost\n",
    "\n",
    "\n",
    "2. Depois, execute o código abaixo para ver se a instalação funcionou corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respective-bosnia",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-marshall",
   "metadata": {},
   "source": [
    "### Versões dos pacotes\n",
    "\n",
    "1. Confira também se os pacotes abaixo estão nas seguintes versões:\n",
    "\n",
    "> Python >= 3.7\n",
    "\n",
    "> numpy >= 1.19.1\n",
    "\n",
    "> SciPy >= 1.5.2\n",
    "\n",
    "> Scikit-Learn >= 0.23.2\n",
    "\n",
    "> XGBoost >= 1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import numpy; print(\"NumPy\", numpy.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n",
    "import xgboost; print(\"XGBoost\", xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-peeing",
   "metadata": {},
   "source": [
    "# Criando algoritmos baselines\n",
    "\n",
    "Antes de analisarmos o algoritmo de Árvore de Decisão, vamos criar aqui algoritmos baseline e compara-los minimanente com o XGBoost, tanto para regressão quanto para classificação. Isso será importante para podermos comparar com os resultados de quando fizermos os ajustes nos hiperparâmetros do XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-salon",
   "metadata": {},
   "source": [
    "### Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes = pd.read_csv('bases/bike_rentals_cleaned.csv', sep=',')\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separando os dados em features e labels\n",
    "X = df_bikes.iloc[:,:-1]\n",
    "y = df_bikes.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos treinar uma regressão linear e comparar com o XGBoost depois\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE: %0.2f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos usar o método describe para tentar qualificar o RMSE\n",
    "df_bikes['cnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xg_reg = XGBRegressor()\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE: %0.2f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-obligation",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "Realizar apenas um teste não é confiável, visto que se você fizer uma divisão dos dados diferente, você obterá diferentes resultados. Para tratar isso, usamos cross-validation, cujo processo já foi explicado na aula de SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para regressão linear\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse = np.sqrt(-scores)\n",
    "print('Reg rmse:', np.round(rmse, 2))\n",
    "print('RMSE mean: %0.2f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para XGBoost\n",
    "model = XGBRegressor()\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse = np.sqrt(-scores)\n",
    "print('Reg rmse:', np.round(rmse, 2))\n",
    "print('RMSE mean: %0.2f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-insulin",
   "metadata": {},
   "source": [
    "### Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None)\n",
    "df_census.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                  'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', \n",
    "                   'income']\n",
    "df_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-lending",
   "metadata": {},
   "source": [
    "Quando olhamos para o dataset, vemos a coluna 'education' e 'education_num'. A última é a versão numérica da primeira. Assim, podemos dropar 'education'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = df_census.drop(['education'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-community",
   "metadata": {},
   "source": [
    "Todas as colunas precisam ser transformadas em colunas numéricas. O método *get_dumies* do pandas pega valores únicos não-numéricos de cada coluna e os converte em colunas, com 1 indicando presença e 0 ausência. Veja o exemplo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemplo\n",
    "pd.get_dummies(df_census['marital-status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = pd.get_dummies(df_census)\n",
    "df_census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-appearance",
   "metadata": {},
   "source": [
    "O objetivo (coluna target) é determinar se uma pessoa ganha mais que 50k ou não. Depois do pd.get_dummies, duas colunas, df_census['income_<=50k'] e df_census['income_>50k] são usadas para determinar se uma pessoa ganha 50k. Visto que ambas representam a mesma coisa, vamos deletar a primeira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = df_census.drop('income_ <=50K', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features e labels\n",
    "X = df_census.iloc[:,:-1]\n",
    "y = df_census.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uma função para executar cross-validation\n",
    "def cross_val(classifier, num_splits=10):\n",
    "    model = classifier\n",
    "    scores = cross_val_score(model, X, y, cv=num_splits)\n",
    "    print('Accuracy:', np.round(scores, 2))\n",
    "    print('Accuracy mean: %0.2f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(XGBClassifier(n_estimators=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-vermont",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Vamos começar executando decision trees para comparar os resultados com o dataset df_census e depois entender como usar GridSearch para encontrar os melhores parâmetros para nossa árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_census.iloc[:,:-1]\n",
    "y = df_census.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = DecisionTreeClassifier(random_state=2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-gregory",
   "metadata": {},
   "source": [
    "Agora vamos treinar uma árvore de regressão usando GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes = pd.read_csv('bases/bike_rentals_cleaned.csv')\n",
    "\n",
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DecisionTreeRegressor(random_state=2)\n",
    "scores = cross_val_score(reg, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse = np.sqrt(-scores)\n",
    "print('RMSE mean: %0.2f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-auction",
   "metadata": {},
   "source": [
    "Podemos checar se o modelo esta overfitando ou não. para isso, podemos predizer o proprio conjunto de treino e verificar o RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_train)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "reg_mse = mean_squared_error(y_train, y_pred)\n",
    "reg_rmse = np.sqrt(reg_mse)\n",
    "reg_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-covering",
   "metadata": {},
   "source": [
    "Como suspeitamos, o modelo está overfitando. Assim, precisamos usar o GridSearchCV para determinar o melhor valor dos hiperparâmetos.\n",
    "\n",
    "Vamos começar por *max_depth*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth':[None,2,3,4,6,8,10,20]}\n",
    "reg = DecisionTreeRegressor(random_state=2)\n",
    "grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_reg.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_reg.best_params_\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e agora verificamos se com esse valor, o RMSE diminui\n",
    "best_model = grid_reg.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, y_pred)**0.5\n",
    "\n",
    "print('Test score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-residence",
   "metadata": {},
   "source": [
    "Para testar com outras features, vamos criar uma função para facilitar nosso trabalho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(params, reg=DecisionTreeRegressor(random_state=2)):\n",
    "\n",
    "    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "    \n",
    "    grid_reg.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_reg.best_params_\n",
    "\n",
    "    print(\"Best params:\", best_params)\n",
    "    \n",
    "    best_score = np.sqrt(-grid_reg.best_score_)\n",
    "\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "    y_pred = grid_reg.predict(X_test)\n",
    "\n",
    "    rmse_test = mean_squared_error(y_test, y_pred)**0.5\n",
    "\n",
    "    print('Test score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos testar para a feature min_samples_leaf\n",
    "grid_search(params={'min_samples_leaf':[1,2,4,6,8,10,20,30]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-ordinance",
   "metadata": {},
   "source": [
    "Vamos ver o que acontece quando usamos as duas features no gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-madison",
   "metadata": {},
   "source": [
    "Surpreendentemente, o score aumentou. Precisamos sempre colocar vários parÂmetros juntos para executar o GridSearch.\n",
    "\n",
    "vamos iniciar min_samples_leaf em 3 e max_depth a partir de 6 e ver o que acontece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(params={'max_depth':[5,6,7,8,9],'min_samples_leaf':[3,5,7,9]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-cedar",
   "metadata": {},
   "source": [
    "Vimos que o score melhorou, mas ainda temos um longo caminho pela frente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-anxiety",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-violation",
   "metadata": {},
   "source": [
    "### Random Forest Classifiers\n",
    "\n",
    "Vamos começar usando o dataset df_census e comparar o Random Forest com Árvores de Decisão e XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_census = df_census.iloc[:,:-1]\n",
    "y_census = df_census.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)\n",
    "\n",
    "#n_estimators = número de árvores\n",
    "#n_jobs = quando igual a -1, usa todos os processadores para execução em paralelo\n",
    "\n",
    "scores = cross_val_score(rf, X_census, y_census, cv=5)\n",
    "print('Accuracy:', np.round(scores, 3))\n",
    "print('Accuracy mean: %0.3f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-backup",
   "metadata": {},
   "source": [
    "A versão default do Random Forest forneceu um resultado melhor que o das Árvores de Decisão (81%), mas não tão bom quanto o XGBoost (86%). \n",
    "\n",
    "Mas aqui, a grande questão é: porque RF foi melhor que uma árvore de decisão individual?\n",
    "\n",
    "A resposta curta é: **bagging**. Com 10 árvores (n_estimator=10), cada predição é baseada em 10 árvores de decisão ao invés de em apenas uma. O Bootstrap age também, fornecendo diversidade e reduzindo a variância.\n",
    "\n",
    "Por padrão, Random Forest Classifiers selecionam a partir da raiz quadrada do número total de features ao fazer um split. Então, se existem 100 features (Colunas), cada árvore de decisão vai considerar apenas 10 features quando fazer o split. Assim, duas árvores com amostras duplicadas podem fornecer predições muito diferentes devido aos diferentes splits. Essa é outra maneira de reduzir variância.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-judges",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "Numa Random Forest Regressor, os exemplos são amostrados com reposição, da mesma maneira que num Random Forest Classifier, mas o número máximo de features é o número total de features ao invés da raiz quadrada. Isto é devido a experimentos. Veja [aqui](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)\n",
    "\n",
    "Além disso, a predição final é feita levando em consideração a média das predições de todas as árvores ao invés de regras baseadas em voto majoritário. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=10, random_state=2, n_jobs=-1)\n",
    "scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-customs",
   "metadata": {},
   "source": [
    "Vamos agora ajustar os hiperparâmetros do Random Forest e aproveitar para introduzir uma nova maneira de fazer GridSearch, chamada de RandomizedSearchCV. A ideia é a mesma por trás do GridSearchCV, porém, como este pode levar muito tempo, RandomizedSearchCV trabalha escolhendo um número aleatório de combinações. Não foi feito para ser exaustivo, mas sim para encontrar a melhor combinação de parâmetros num tempo limitado.\n",
    "\n",
    "\n",
    "Vamos voltar a dataset df_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)\n",
    "scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\n",
    "rmse = np.sqrt(-scores)\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-treasurer",
   "metadata": {},
   "source": [
    "Introduzimos um novo parâmetro aqui: **warm_start**.\n",
    "\n",
    "Quando warm_star=True, adicionar mais árvores não requer começar tudo do zero. Se você mudar n_stimators de 100 para 200, Random Forest vai reusar a solução do fit() anterior, tornando a execução mais rápida. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-thesis",
   "metadata": {},
   "source": [
    "Agora vamos usar o RandomizedSearchCV. Para isso, vamos criar uma função que recebe os parâmetros que eu quero otimizar, a quantidade de vezes que vamos executar o algoritmo e o regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "def randomized_search_reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):\n",
    "    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error', \n",
    "                                  cv=10, n_jobs=-1, random_state=2)\n",
    "    \n",
    "    rand_reg.fit(X_train, y_train)\n",
    "\n",
    "    best_model = rand_reg.best_estimator_\n",
    "\n",
    "    best_params = rand_reg.best_params_\n",
    "\n",
    "    print(\"Best params:\", best_params)\n",
    "    \n",
    "    best_score = np.sqrt(-rand_reg.best_score_)\n",
    "\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    rmse_test = MSE(y_test, y_pred)**0.5\n",
    "    \n",
    "    print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agora executamos\n",
    "\n",
    "randomized_search_reg(params={\n",
    "                          'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "                          'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "                          'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "                          'max_depth':[None,4,6,8,10,12,15,20]\n",
    "                         }, runs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-bulgarian",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-steps",
   "metadata": {},
   "source": [
    "Para entendermos a maneira correra que o Gradient Boosting funciona, vamos usar uma Árvore de Regressão. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-watson",
   "metadata": {},
   "source": [
    "1. Aqui, vamos usar um base learner, então configuramos max_depth=2, pois queremos aprender a partir dos erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos treinar um Decision tree Regressor\n",
    "tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-organization",
   "metadata": {},
   "source": [
    "2. Vamos fazer predição com o conjunto de treino: ao invés de fazer predições com o conjunto de teste, no gradient boosting as predições são feitas inicialmente com o conjunto de treino. Porque? Para comparar os residuals, precisamos comparar as predições enquanto estamos na fase de treino. A fase de teste do modelo acontece no final, quando todas as árvores estiverem construídas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = tree_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-investment",
   "metadata": {},
   "source": [
    "3. Agora, calculamos os residuals: a diferença entre o predito e o observado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train = y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-southeast",
   "metadata": {},
   "source": [
    "**NOTA**: os residuals são definidos como y2_train por que agora eles são as labels da próxima árvore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-alaska",
   "metadata": {},
   "source": [
    "4. Treine uma nova árvore em cima dos residuals. Isso vai, gradativamente, diminuindo os residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_2.fit(X_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-dutch",
   "metadata": {},
   "source": [
    "5. Repita os passos 2 a 4. Conforme o processo continua, os residuals gradativamente vão se aproximando de 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train_pred = tree_2.predict(X_train)\n",
    "y3_train = y2_train - y2_train_pred\n",
    "\n",
    "\n",
    "tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_3.fit(X_train, y3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-variable",
   "metadata": {},
   "source": [
    "6. Agora somamos os resultados: somar os resultados requer fazer predições para cada árvore com o conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = tree_1.predict(X_test)\n",
    "\n",
    "y2_pred = tree_2.predict(X_test)\n",
    "\n",
    "y3_pred = tree_3.predict(X_test)\n",
    "\n",
    "y_pred = y1_pred + y2_pred + y3_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-south",
   "metadata": {},
   "source": [
    "7. Agora, podemos calcular o MSE para obter os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(y_test, y_pred)**0.5\n",
    "#um resultado excelente para um weak learner, principalmente quando comparamos com o resultado da \n",
    "#árvore de regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-volunteer",
   "metadata": {},
   "source": [
    "### Gradient Boosting usando sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-modification",
   "metadata": {},
   "source": [
    "Podemos treinar um Gradiet Boosting usando a scikit-learn para ver se chegamos no mesmo resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-seminar",
   "metadata": {},
   "source": [
    "Temos um novo parâmetro: learning_rate. Para entender sua influência (e também no XGBoost), vamos treinar dois GBs mudando somente *n_estimators*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-optimization",
   "metadata": {},
   "source": [
    "Uma melhora significativa. Vamos testar com 300, então."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-sleeping",
   "metadata": {},
   "source": [
    "Opa! Algo de errado não está certo. Aqui vemos a importância do learning_rate. \n",
    "\n",
    "Basicamente, a ideia consiste em reduzir a contribuição de árvores individuais de modo que nenhuma árvore tenha muita influência ao construir o modelo, ou seja, o learning rate limita a influência de árvores individuais. Assim, se diminuirmos seu valor, veremos uma grande diferença:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=0.1) #default sklearn\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-mistress",
   "metadata": {},
   "source": [
    "Agora, podemos executar um RandomizedSearchCV e encontrar os melhores parâmetros (Dentro de um range específico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'subsample':[0.65, 0.7, 0.75],\n",
    "                          'n_estimators':[300, 500, 1000],\n",
    "                          'learning_rate':[0.05, 0.075, 0.1]\n",
    "       }\n",
    "gbr = GradientBoostingRegressor(max_depth=3, random_state=2)\n",
    "\n",
    "rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', \n",
    "                              cv=5, n_jobs=-1, random_state=2)\n",
    "\n",
    "rand_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = rand_reg.best_estimator_\n",
    "best_params = rand_reg.best_params_\n",
    "print(\"Best params:\", best_params)\n",
    "best_score = np.sqrt(-rand_reg.best_score_)\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-trigger",
   "metadata": {},
   "source": [
    "**NOTA**: usamos um parâmetro a mais: subsample. Como o nome sugere, é um subconjunto de amostras. Isso signifca que nem todas as linhas serão incluídas ao construir uma árvore. Quando seu valor é diferente de 1.0, o modelo é classificado como **Stochastic Gradient Descent**, em que *stochastic* significa alguma aleatoriedade é inerente ao modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-flavor",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-tamil",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', \n",
    "                    learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print('Score: ' + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-intranet",
   "metadata": {},
   "source": [
    "Algumas considerações sobre os parâmetros:\n",
    "\n",
    "1. **booster='gbtree'**: booster é o *base learner*. 'gbtree' é o gradient boosted tree\n",
    "\n",
    "2. **objective='multi:softprob'**: função objetivo. No caso da classificação, calcula a probabilidade de cada classe e escolhe aquela com maior valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-malaysia",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "X,y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', \n",
    "                    learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse = np.sqrt(-scores)\n",
    "print('RMSE:', np.round(rmse, 3))\n",
    "print('RMSE mean: %0.3f' % (rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-gibraltar",
   "metadata": {},
   "source": [
    "1. **objective='reg:squarederror'**: é o MSE a partir do qual derivamos a função objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para poder comparar o resultado, podemos verificar usar o método describe na variável y (label) e obter \n",
    "#suas principais estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-transaction",
   "metadata": {},
   "source": [
    "O resultado de 63.124 é menor que 1 desvio padrão, o que é um resultado respeitável. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-castle",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Agora, temos um template tanto para classificação quanto para regressão que podemos usar como ponto de partida para resolver problemas usando XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-hammer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
